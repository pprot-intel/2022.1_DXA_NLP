{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Interactive question answering with OpenVINO\n",
    "\n",
    "Goal of this task is to complete a demo that showcases interactive question answering with OpenVINO. We will use [small BERT-large-like model](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/bert-small-uncased-whole-word-masking-squad-int8-0002) distilled and quantized to INT8 on SQuAD v1.1 training set from larger BERT-large model. The model comes from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/).\n",
    "\n",
    "At the bottom of this notebook, you will see live inference results from your inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from openvino.runtime import Core, Dimension\n",
    "\n",
    "import tokens_bert as tokens\n",
    "from data_processing import prepare_input, postprocess, load_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The model\n",
    "\n",
    "### Download the model\n",
    "\n",
    "We use `omz_downloader`, which is a command-line tool from the `openvino-dev` package. `omz_downloader` automatically creates a directory structure and downloads the selected model. These models are already converted to OpenVINO Intermediate Representation (IR) - if they weren't, we'd need to use `omz_converter` first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# directory where model will be downloaded\n",
    "output_model_dir = \"model\"\n",
    "cache_model_dir = \"model\"\n",
    "\n",
    "# desired precision\n",
    "precision = \"FP16-INT8\"\n",
    "\n",
    "# model name as named in Open Model Zoo\n",
    "model_name = \"bert-small-uncased-whole-word-masking-squad-int8-0002\"\n",
    "\n",
    "model_path = f\"model/intel/{model_name}/{precision}/{model_name}.xml\"\n",
    "model_weights_path = f\"model/intel/{model_name}/{precision}/{model_name}.bin\"\n",
    "vocab_file_path = \"data/vocab.txt\"\n",
    "\n",
    "# create dictionary with words and their indices\n",
    "vocab = tokens.load_vocab_file(vocab_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_command = f\"omz_downloader \" \\\n",
    "                   f\"--name {model_name} \" \\\n",
    "                   f\"--precision {precision} \" \\\n",
    "                   f\"--output_dir {output_model_dir} \" \\\n",
    "                   f\"--cache_dir {cache_model_dir}\"\n",
    "! $download_command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "Downloaded models are located in a fixed structure, which indicates vendor, model name and precision. Only a few lines of code are required to run the model:\n",
    "\n",
    "- First, we create an Inference Engine object\n",
    "- Then we read the network architecture and model weights from the .xml and .bin files\n",
    "- Next, we want to change input shapes so that length of text (on each layer) is no longer static\n",
    "- Finally, we compile the network for the desired device. OpenVINO currently supports dynamic shapes only with `CPU`.\n",
    "\n",
    "Replace `pass` with correct code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize inference engine\n",
    "ie_core = Core()\n",
    "\n",
    "# read the model and corresponding weights from file\n",
    "model = ie_core.read_model(model=model_path, weights=model_weights_path)\n",
    "\n",
    "# assign dynamic shapes to every input layer\n",
    "# shape for each layer is [1, 384], where 1 stands for batch size, 384 stands for length\n",
    "# we need to replace length of text for each layer to be dynamic\n",
    "\n",
    "for input_layer in model.inputs:\n",
    "    pass\n",
    "\n",
    "# compile the model for the CPU\n",
    "compiled_model = ie_core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "\n",
    "input_keys = list(compiled_model.inputs)\n",
    "output_keys = list(compiled_model.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Input keys are the names of the input nodes and output keys contain names of output nodes of the network. In the case of the BERT-large-like model, we have four inputs and two outputs.\n",
    "\n",
    "Replace `None` and `____` with correct code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "[i.any_name for i in input_keys], [o.any_name for o in output_keys]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, the data processing happens. You can find it in `data_processing.py`.\n",
    "\n",
    "Next, it's time to create inference:\n",
    "\n",
    "- Firstly, we need to create a list of tokens from the context and the question\n",
    "- Then, we are looking for the best answer in the context. The best answer should come with the highest score.\n",
    "\n",
    "Replace `None` and `____` with correct code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_best_answer(question, context, vocab, input_keys):\n",
    "    # convert context string to tokens\n",
    "    context_tokens, context_tokens_start_end = tokens.text_to_tokens(\n",
    "        text=context.lower(), vocab=vocab\n",
    "    )\n",
    "    # convert question string to tokens\n",
    "    question_tokens, _ = tokens.text_to_tokens(text=question.lower(), vocab=vocab)\n",
    "\n",
    "    network_input = prepare_input(question_tokens, context_tokens, input_keys)\n",
    "    input_size = len(context_tokens) + len(question_tokens) + 3\n",
    "    \n",
    "    # Create inference request\n",
    "    request = None\n",
    "\n",
    "    # Run inference using network_input\n",
    "    request.____\n",
    "\n",
    "    # Get output tensors from inference in numpy format\n",
    "    output_start = request.____\n",
    "    output_end = request.____\n",
    "\n",
    "    # postprocess the result getting the score and context range for the answer\n",
    "    score_start_end = postprocess(output_start=output_start,\n",
    "                                  output_end=output_end,\n",
    "                                  question_tokens=question_tokens,\n",
    "                                  context_tokens_start_end=context_tokens_start_end,\n",
    "                                  input_size=input_size)\n",
    "\n",
    "    # return the part of the context, which is already an answer\n",
    "    return context[score_start_end[1]:score_start_end[2]], score_start_end[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Main Processing Function\n",
    "\n",
    "Run question answering on specific knowledge base and iterate through the questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "test_replace": {
     "input()": "\"What does OpenVINO mean?\"",
     "while True": "for i in range(1)"
    }
   },
   "outputs": [],
   "source": [
    "def run_question_answering(sources, vocab):\n",
    "    print(f\"Context: {sources}\", flush=True)\n",
    "    context = load_context(sources)\n",
    "\n",
    "    if len(context) == 0:\n",
    "        print(\"Error: Empty context or outside paragraphs\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        question = input()\n",
    "        # if no question - break\n",
    "        if question == \"\":\n",
    "            break\n",
    "\n",
    "        # measure processing time\n",
    "        start_time = time.perf_counter()\n",
    "        answer, score = get_best_answer(\n",
    "            question=question, context=context, vocab=vocab, input_keys=input_keys\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f\"Score: {score:.2f}\")\n",
    "        print(f\"Time: {end_time - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run\n",
    "\n",
    "### Run on local paragraphs\n",
    "\n",
    "Change sources to your own to answer your questions. You can use as many sources as you want. Usually, you need to wait a few seconds for the answer, but the longer context the longer the waiting time. The model is very limited and sensitive for the input. The answer can depend on whether there is a question mark at the end. The model will try to answer any of your questions even there is no good answer in the context, so in that case, you can see random results.\n",
    "\n",
    "Sample source: Computational complexity theory paragraph (from [here](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Computational_complexity_theory.html))\n",
    "\n",
    "Sample questions:\n",
    "\n",
    "- What is the term for a task that generally lends itself to being solved by a computer?\n",
    "- By what main attribute are computational problems classified utilizing computational complexity theory?\n",
    "- What branch of theoretical computer science deals with broadly classifying computational problems by difficulty and class of relationship?\n",
    "\n",
    "If you want to stop the processing just put an empty string.\n",
    "\n",
    "_Note: Firstly, run the code below and then put your questions in the box._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sources = [\n",
    "    \"Computational complexity theory is a branch of the theory of computation in theoretical computer \"\n",
    "    \"science that focuses on classifying computational problems according to their inherent difficulty, \"\n",
    "    \"and relating those classes to each other. A computational problem is understood to be a task that \"\n",
    "    \"is in principle amenable to being solved by a computer, which is equivalent to stating that the \"\n",
    "    \"problem may be solved by mechanical application of mathematical steps, such as an algorithm.\"\n",
    "]\n",
    "\n",
    "run_question_answering(sources, vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Run on websites\n",
    "\n",
    "You can also provide urls. Note that the context (knowledge base) is built from website paragraphs. If some information is outside the paragraphs, the algorithm won't able to find it.\n",
    "\n",
    "Sample source: [OpenVINO wiki](https://en.wikipedia.org/wiki/OpenVINO)\n",
    "\n",
    "Sample questions:\n",
    "\n",
    "- What does OpenVINO mean?\n",
    "- What is the license for OpenVINO?\n",
    "- Where can you deploy OpenVINO code?\n",
    "\n",
    "If you want to stop the processing just put an empty string.\n",
    "\n",
    "_Note: Firstly, run the code below and then put your questions in the box._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sources = [\"https://en.wikipedia.org/wiki/OpenVINO\"]\n",
    "\n",
    "run_question_answering(sources, vocab)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d8915318276cc9e734a329d09d8937c92b85a8f5b670b060a3ed42bccd69b00"
  },
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
